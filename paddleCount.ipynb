{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paddleCount.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FAnTB70cHVoW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle\n",
        "# !pip install paddlepaddle-gpu==0.14.0.post87\n",
        "# !export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/lib/:/usr/local/lib/libmkldnn.so.0:/usr/local/lib/libmklml_intel.so\"\n",
        "!ldconfig\n",
        "# !export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/lib\"\n",
        "import contextlib\n",
        "from __future__ import print_function, division\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import paddle.v2 as paddle\n",
        "import paddle.fluid as fluid\n",
        "import paddle.fluid.framework as framework\n",
        "import paddle.fluid.layers as pd\n",
        "from paddle.fluid.executor import Executor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bsqJ_N08AXG8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Unzip data\"\"\"\n",
        "\n",
        "!tar -xf baidu_star_2018_train_stage1_new.tar\n",
        "!mkdir baidu_star_2018/image/preprocessed\n",
        "!ls baidu_star_2018/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fww_mAOwAdUs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Data pre-processing\n",
        "\n",
        "    An image is resized to a certain height and split into many ribbons with the same width.\n",
        "    In a ribbon, any appearing person is assigned to a corresponding slot.\n",
        "    There is a label for every ribbon. Elements from the lower half of the label indicate \n",
        "    the probability of containing at least one target in that slot, while the high half indicates\n",
        "    the number of targets in the slot.\n",
        "\n",
        "    'train_names' and 'test_names' store names of split, processed ribbon files\n",
        "    'data_dict' stores labels keyed by file names\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "from PIL import Image, ImageFilter, ImageOps, ImageEnhance\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "height = 640\n",
        "crop_width = 112\n",
        "stride = int(crop_width/2)\n",
        "label_slots = 10\n",
        "update_data = True\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with open('baidu_star_2018/annotation/annotation_train_stage1.json', 'r') as json_f:\n",
        "    annotations_array = json.load(json_f)['annotations']\n",
        "random.shuffle(annotations_array)\n",
        "for item in annotations_array:\n",
        "  item['name'] = item['name'][13:]\n",
        "train_names = []\n",
        "test_names = []\n",
        "\n",
        "if update_data:\n",
        "  data_dict = {}\n",
        "def process_data(anarr):\n",
        "  def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "  print('processing data')\n",
        "  out = display(progress(0, 100), display_id=True)\n",
        "  prog = 0\n",
        "  length_a = len(anarr)\n",
        "  space = int(round((crop_width - stride)/2))\n",
        "  for item in anarr:\n",
        "    img = Image.open('baidu_star_2018/image/stage1/train/'+item['name'])\n",
        "    img = np.asarray(img)\n",
        "    for region in item['ignore_region']:\n",
        "      ignore_points = []\n",
        "      for point in region:\n",
        "          ignore_points.append([point['x'], point['y']])\n",
        "      cv2.fillPoly(img, np.array([ignore_points]), np.mean(img,axis=(0,1)))\n",
        "    img = Image.fromarray(img)\n",
        "    w,h = img.size\n",
        "    scale = height/h\n",
        "    img = img.resize((int(scale*w), int(scale*h)+1),Image.ANTIALIAS)\n",
        "    anns = []\n",
        "    for ann in item['annotation']:\n",
        "      x = int(scale*ann['x'])\n",
        "      y = int(scale*ann['y'])\n",
        "      if item['type'] == 'bbox':\n",
        "        pw = scale*ann['w']\n",
        "        ph = scale*ann['h']\n",
        "        x = x + int(pw/2)\n",
        "        y = y + int(0.1*ph)\n",
        "      anns.append((x,y))\n",
        "    w2,h2 = img.size\n",
        "    sw = int(w2/stride)\n",
        "    \n",
        "    modalities = []\n",
        "    modalities.append(img)\n",
        "    modalities.append(ImageEnhance.Color(img).enhance(random.randint(1,10)/10))\n",
        "    \n",
        "    for mod in range(len(modalities)):\n",
        "      mod_img = modalities[mod]\n",
        "\n",
        "      rands = random.randint(stride,int(crop_width*0.8))\n",
        "      for s in range(sw):\n",
        "        shift = stride*s - rands\n",
        "        box = (shift, 0, shift+crop_width, height)\n",
        "        mid_box = (space, 0, space+stride, height)\n",
        "        crop_anns = []\n",
        "        label = [0,0] * label_slots\n",
        "        for ann2 in anns:\n",
        "          if ann2[0] >= shift + space and ann2[0] < shift + crop_width - space:\n",
        "            y = ann2[1]\n",
        "            pos = int(label_slots*y/height)\n",
        "            label[pos] = 1\n",
        "            label[pos+10] += 1\n",
        "        img_crop = mod_img.crop(box)\n",
        "        for mirror in ['m','o']:\n",
        "          for flip in ['f', 'p']:\n",
        "            img_crop_mf = img_crop\n",
        "            img_crop_label = np.array(label)\n",
        "            if mirror == 'm':\n",
        "              img_crop_mf = ImageOps.mirror(img_crop_mf)\n",
        "            if flip == 'f':\n",
        "              img_crop_mf = ImageOps.flip(img_crop_mf)\n",
        "              img_crop_label = np.reshape(img_crop_label, [-1,2])\n",
        "              img_crop_label = np.flip(img_crop_label, 0)\n",
        "              img_crop_label = np.reshape(img_crop_label, [-1])\n",
        "\n",
        "            crop_name = str(s) + 'm' + str(mod) + mirror + flip + item['name']\n",
        "            data_dict[crop_name] = img_crop_label\n",
        "            \"\"\"darken two sides\"\"\"\n",
        "            img_crop_mid = img_crop_mf.crop(mid_box)\n",
        "            img_crop_mf = img_crop_mf.point(lambda p: p*0.6)\n",
        "            img_crop_mf.paste(img_crop_mid, mid_box)\n",
        "\n",
        "            img_crop_mf.save('baidu_star_2018/image/preprocessed/'+crop_name)\n",
        "            if prog < length_a * 0.85:\n",
        "              train_names.append(crop_name)\n",
        "            else:\n",
        "              test_names.append(crop_name)\n",
        "        \n",
        "    if prog % 17 ==0:\n",
        "      percent =  (prog+1) / length_a * 100\n",
        "      out.update(progress(percent, 100))\n",
        "    prog += 1\n",
        "if update_data:\n",
        "  process_data(annotations_array)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print('finished in', str(round(elapsed_time,2))+'s')\n",
        "annotations_array = None\n",
        "random.shuffle(train_names)\n",
        "random.shuffle(test_names)\n",
        "import pickle\n",
        "with open('data_dict.pkl', 'wb') as f:\n",
        "  pickle.dump([data_dict, train_names, test_names], f)\n",
        "print('data: ', len(data_dict))\n",
        "print('training data: ', len(train_names))\n",
        "print('test data: ', len(test_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VUyOvtjBNmBH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Print a ribbon\"\"\"\n",
        "\n",
        "img = Image.open('baidu_star_2018_test_stage1/baidu_star_2018/image/preprocessed/'+'2a2c372ac00bd4c3e9dc683b7969c07bd.jpg')\n",
        "img = np.asarray(img)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(test_data_dict['2a2c372ac00bd4c3e9dc683b7969c07bd.jpg'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAy9Iy-lKLwt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Hyper parameters\"\"\"\n",
        "batch_size = 100\n",
        "l2_lambda = 0.0001\n",
        "beta1 = 1\n",
        "beta2 = 1\n",
        "beta3 = 2\n",
        "alpha = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W97hA6gvAQO1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Training data batches\"\"\"\n",
        "\n",
        "def MaxMinNormalization(x):\n",
        "      Min = np.min(x)\n",
        "      Max = np.max(x)\n",
        "      if Max != Min:\n",
        "        return (x - Min) / (Max - Min)\n",
        "      else:\n",
        "        return x\n",
        "def train_data():\n",
        "  for name in train_names:\n",
        "    img = Image.open('baidu_star_2018/image/preprocessed/'+name)\n",
        "    img = np.asarray(img)\n",
        "    img = MaxMinNormalization(img)\n",
        "    label = data_dict[name]\n",
        "    yield img, label\n",
        "def test_data():\n",
        "  for name in test_names:\n",
        "    img = Image.open('baidu_star_2018/image/preprocessed/'+name)\n",
        "    img = np.asarray(img)\n",
        "    img = MaxMinNormalization(img)\n",
        "    label = data_dict[name]\n",
        "    yield img, label\n",
        "    \n",
        "train_reader = paddle.batch(\n",
        "    train_data,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_reader = paddle.batch(\n",
        "    test_data,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mCtRbylHgFw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "c60a492c-75eb-4fbe-dbcd-f6e31ddeaf47",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1533238934625,
          "user_tz": -60,
          "elapsed": 1633,
          "user": {
            "displayName": "George Chen",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112767084976809705964"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Define the model\n",
        "\n",
        "    This model has a inception structure, though its configuration is a lot \n",
        "    different from the one introduced in the GoogLeNet paper.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def conv(kernel_size, input, filters, padding=0, strides=(1,1)):\n",
        "  return pd.conv2d(input, filters, kernel_size,\n",
        "                          stride = strides,\n",
        "                          act='relu',\n",
        "                          padding=padding,\n",
        "                          param_attr=fluid.initializer.Xavier(),\n",
        "                          bias_attr=fluid.initializer.Constant(0.0))\n",
        "def dense(input, output_size, act='sigmoid'):\n",
        "  return pd.fc(input, output_size,\n",
        "                          act=act,\n",
        "                          param_attr=fluid.initializer.Xavier(),\n",
        "                          bias_attr=fluid.initializer.Constant(0.0))\n",
        "  \n",
        "  \n",
        "def inception_unit(input, output_size):\n",
        "    block_size = int(output_size/4)\n",
        "    \n",
        "    incep_1_conv_1x1_1 = conv(1, input, block_size)\n",
        "\n",
        "    incep_1_conv_1x1_2 = conv(1, input, block_size)\n",
        "\n",
        "    incep_1_conv_1x1_3 = conv(1, input, block_size)\n",
        "\n",
        "    incep_1_max_p = pd.pool2d(input, pool_size=3, pool_stride=1, pool_padding=1)\n",
        "\n",
        "    incep_1_conv_1x1_4 = conv(1, incep_1_max_p, block_size)\n",
        "\n",
        "    incep_1_conv_3x3 = conv(3, incep_1_conv_1x1_2, block_size, padding=1)\n",
        "    \n",
        "    incep_1_conv_5x5 = conv(5, incep_1_conv_1x1_3, block_size, padding=2)\n",
        "\n",
        "    concat = pd.concat([incep_1_conv_1x1_1, incep_1_conv_1x1_4, incep_1_conv_3x3, incep_1_conv_5x5], 1, name='inception_unit')\n",
        "\n",
        "    return concat\n",
        "  \n",
        "def conv_pool(input, strides=(2,2)):\n",
        "  block_size = int(input.shape[-3]/2)\n",
        "\n",
        "  conv_pool_conv_1x1_1 = conv(1, input, block_size)\n",
        "  \n",
        "  conv_pool_conv_1x1_2 = conv(1, input, block_size)\n",
        "  \n",
        "  conv_pool_conv_3x3_1 = conv(3, conv_pool_conv_1x1_1, block_size, padding=1)\n",
        "  \n",
        "  conv_pool_conv_3x3_2 = conv(3, conv_pool_conv_1x1_2, block_size, strides=strides, padding=1)\n",
        "  \n",
        "  conv_pool_conv_3x3_3 = conv(3, conv_pool_conv_3x3_1, block_size, strides=strides, padding=1)\n",
        "  \n",
        "  max_pool = pd.pool2d(input, pool_size=3, pool_stride=strides, pool_padding=(1,1))\n",
        "  \n",
        "  concat = pd.concat([conv_pool_conv_3x3_2, conv_pool_conv_3x3_3, max_pool], 1, name='conv_pool')\n",
        "\n",
        "  return concat\n",
        "\n",
        "def inception_cnn(input, is_test):\n",
        "\n",
        "  a = conv(3, input, 16, padding=1)\n",
        "  print('1', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv(3, a, 32, padding=1)\n",
        "  print('2', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,2))\n",
        "  print('3', a.shape)\n",
        "  a = inception_unit(a, 64)\n",
        "  print('4', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,2))\n",
        "  print('5', a.shape)\n",
        "  a = inception_unit(a, 64)\n",
        "  print('6', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,2))\n",
        "  print('7', a.shape)\n",
        "  a = inception_unit(a, 128)\n",
        "  print('8', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,2))\n",
        "  print('9', a.shape)\n",
        "  a = inception_unit(a, 128)\n",
        "  print('10', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,1))\n",
        "  print('11', a.shape)\n",
        "  a = inception_unit(a, 128)\n",
        "  print('12', a.shape)\n",
        "  bn = pd.batch_norm(a, is_test=is_test)\n",
        "  a = conv_pool(a, (2,1))\n",
        "  print('13', a.shape)\n",
        "  a = inception_unit(a, 128)\n",
        "  print('14', a.shape)\n",
        "  a = conv_pool(a, (2,1))\n",
        "  print('15', a.shape)\n",
        "  d = dense(a, 2048, act='leaky_relu')\n",
        "  d = pd.dropout(d, 0.5, is_test=is_test)\n",
        "  d = dense(d, 1024)\n",
        "  print('16', d.shape)\n",
        "  return d\n",
        "\n",
        "def model(is_test=False):\n",
        "    \"\"\"input: [batch_size, channels, width, height]\"\"\"\n",
        "\n",
        "    input = fluid.layers.data(name='input', shape=[3, height, crop_width], dtype='float32')\n",
        "    \n",
        "    cnn_output = inception_cnn(input, is_test)\n",
        "    print('cnn output:', cnn_output.shape)\n",
        "\n",
        "    pred_nums_0 = dense(cnn_output, 256)\n",
        "    pred_nums = dense(pred_nums_0, label_slots, act=None) # linear\n",
        "    \n",
        "    pred_probs_0 = dense(cnn_output, 256)\n",
        "    pred_probs = dense(pred_probs_0, label_slots) # logistic\n",
        "\n",
        "    return pred_probs, pred_nums\n",
        "\n",
        "def train_program():\n",
        "    labels = fluid.layers.data(name='labels', shape=[1], dtype='float32')\n",
        "    \n",
        "    pred_probs, pred_nums = model()\n",
        "  \n",
        "    label_nums = pd.slice(labels, axes = [0, 1], starts=[0,label_slots], ends=[9999,9999])\n",
        "    num_loss = pd.reduce_mean(pd.reduce_sum(pd.square_error_cost(label_nums, pred_nums), dim=1))\n",
        "\n",
        "    label_probs = pd.slice(labels, axes = [0, 1], starts=[0,0], ends=[9999,label_slots])\n",
        "    pred_probs = pd.clip(pred_probs, min=1e-10, max=1- (1e-10))\n",
        "    probs_loss = pd.reduce_mean(pd.reduce_sum((-1) * label_probs * pd.log(pred_probs) - (1-label_probs) * pd.log(1 - pred_probs), dim=1 ))\n",
        "\n",
        "    count_loss = pd.reduce_mean(pd.reduce_sum(pd.abs(label_probs*label_nums - pred_probs*pred_nums), dim=1))\n",
        "    \n",
        "    loss = num_loss * beta1 + probs_loss * beta2 + count_loss * beta3\n",
        "    \n",
        "    return loss\n",
        "  \n",
        "def optimizer_program():\n",
        "    return fluid.optimizer.Adam(learning_rate=alpha)\n",
        "\n",
        "use_cuda = False\n",
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "\n",
        "trainer = fluid.Trainer(\n",
        "    train_func=train_program,\n",
        "    place=place,\n",
        "    optimizer_func=optimizer_program)\n",
        "feed_order=['input', 'labels']\n",
        "\n",
        "# Specify the directory to save the parameters\n",
        "params_dirname = \"a.model\"\n",
        "\n",
        "# Plot data\n",
        "from paddle.v2.plot import Ploter\n",
        "train_title = \"Train cost\"\n",
        "test_title = \"Test cost\"\n",
        "plot_cost = Ploter(train_title, test_title)\n",
        "\n",
        "step = 0\n",
        "\n",
        "# event_handler prints training and testing info\n",
        "def event_handler_plot(event):\n",
        "    global step\n",
        "    if isinstance(event, fluid.EndStepEvent):\n",
        "        if event.step % 50 == 0: #record a test cost every 10 batches\n",
        "            test_metrics = trainer.test(\n",
        "                reader=test_reader, feed_order=feed_order)\n",
        "\n",
        "            plot_cost.append(test_title, step, test_metrics[0])\n",
        "            plot_cost.plot()\n",
        "\n",
        "            if test_metrics[0] < 0.1:\n",
        "                # If the accuracy is good enough, we can stop the training.\n",
        "                print('loss is less than 0.1, stop')\n",
        "                trainer.stop()\n",
        "\n",
        "        # We can save the trained parameters for the inferences later\n",
        "        if params_dirname is not None:\n",
        "            trainer.save_params(params_dirname)\n",
        "\n",
        "        step += 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 (-1L, 16L, 640L, 112L)\n",
            "2 (-1L, 32L, 640L, 112L)\n",
            "3 (-1L, 64L, 320L, 56L)\n",
            "4 (-1L, 64L, 320L, 56L)\n",
            "5 (-1L, 128L, 160L, 28L)\n",
            "6 (-1L, 64L, 160L, 28L)\n",
            "7 (-1L, 128L, 80L, 14L)\n",
            "8 (-1L, 128L, 80L, 14L)\n",
            "9 (-1L, 256L, 40L, 7L)\n",
            "10 (-1L, 128L, 40L, 7L)\n",
            "11 (-1L, 256L, 20L, 7L)\n",
            "12 (-1L, 128L, 20L, 7L)\n",
            "13 (-1L, 256L, 10L, 7L)\n",
            "14 (-1L, 128L, 10L, 7L)\n",
            "15 (-1L, 256L, 5L, 7L)\n",
            "16 (-1L, 1024L)\n",
            "cnn output: (-1L, 1024L)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H4-6nkrSICBf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Training\"\"\"\n",
        "\n",
        "trainer.train(\n",
        "    reader=train_reader,\n",
        "    num_epochs=10,\n",
        "    event_handler=event_handler_plot,\n",
        "    feed_order=feed_order)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxV4SrQK_2cz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "\n",
        "inferencer = fluid.Inferencer(\n",
        "    infer_func=model,\n",
        "    param_path=params_dirname,\n",
        "    place=place)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2meyugoM6ke",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"unzip test data\"\"\"\n",
        "\n",
        "!unzip baidu_star_2018_test_stage1.zip\n",
        "!ls baidu_star_2018_test_stage1/baidu_star_2018\n",
        "!mkdir baidu_star_2018_test_stage1/baidu_star_2018/image/preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lf3DPvbkM-4l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Preparing data for test\"\"\"\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image, ImageFilter\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "height = 640\n",
        "crop_width = 112\n",
        "stride = int(crop_width/2)\n",
        "update_data = True\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with open('baidu_star_2018_test_stage1/baidu_star_2018/annotation/annotation_test_stage1.json', 'r') as json_f:\n",
        "    annotations_array = json.load(json_f)['annotations']\n",
        "\n",
        "for item in annotations_array:\n",
        "  item['name'] = item['name'][12:]\n",
        "\n",
        "if update_data:\n",
        "  test_data_dict = {}\n",
        "  test_data_names = []\n",
        "def process_data(anarr):\n",
        "  def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "  print('processing data')\n",
        "  out = display(progress(0, 100), display_id=True)\n",
        "  prog = 0\n",
        "  length_a = len(anarr)\n",
        "  space = int(round((crop_width - stride)/2))\n",
        "  for item in anarr:\n",
        "    img = Image.open('baidu_star_2018_test_stage1/baidu_star_2018/image/stage1/test/'+item['name'])\n",
        "    \n",
        "    img = np.asarray(img)\n",
        "    for region in item['ignore_region']:\n",
        "      ignore_points = []\n",
        "      for point in region:\n",
        "          ignore_points.append([point['x'], point['y']])\n",
        "      cv2.fillPoly(img, np.array([ignore_points]), np.mean(img,axis=(0,1)))\n",
        "    img = Image.fromarray(img)\n",
        "    \n",
        "    w,h = img.size\n",
        "    scale = height/h\n",
        "    img = img.resize((int(scale*w), int(scale*h)+1),Image.ANTIALIAS)\n",
        "\n",
        "    w2,h2 = img.size\n",
        "    sw = int(w2/stride)\n",
        "    rands = random.randint(stride,int(crop_width*0.8))\n",
        "    \n",
        "    for s in range(sw):\n",
        "      shift = stride*s - rands\n",
        "      box = (shift, 0, shift+crop_width, height)\n",
        "      mid_box = (space, 0, space+stride, height)\n",
        "      img_crop = img.crop(box)\n",
        "      \"\"\"darken two sides\"\"\"\n",
        "      img_crop_mid = img_crop.crop(mid_box)\n",
        "      img_crop = img_crop.point(lambda p: p*0.6)\n",
        "      img_crop.paste(img_crop_mid, mid_box)\n",
        "      crop_name = str(s)+item['name']\n",
        "      img_crop.save('baidu_star_2018_test_stage1/baidu_star_2018/image/preprocessed/'+crop_name)\n",
        "      test_data_dict[crop_name] = {\n",
        "          'label': [],\n",
        "          'id': item['id']\n",
        "      }\n",
        "      test_data_names.append(crop_name)\n",
        "    if prog % 17 ==0:\n",
        "      percent =  (prog+1) / length_a * 100\n",
        "      out.update(progress(percent, 100))\n",
        "    prog += 1\n",
        "if update_data:\n",
        "  process_data(annotations_array)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print('finished in', str(round(elapsed_time,2))+'s')\n",
        "annotations_array = None\n",
        "import pickle\n",
        "with open('test_data_dict.pkl', 'wb') as f:\n",
        "  pickle.dump([test_data_dict, test_data_names], f)\n",
        "print('data: ', len(test_data_dict))\n",
        "print('test data: ', len(test_data_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2PVctSMIKe19",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Function for reading a test batch\"\"\"\n",
        "\n",
        "current_batch = 0\n",
        "def MaxMinNormalization(x):\n",
        "  Min = np.min(x)\n",
        "  Max = np.max(x)\n",
        "  if Max != Min:\n",
        "    return (x - Min) / (Max - Min)\n",
        "  else:\n",
        "    return x\n",
        "def next_batch(batch_size):\n",
        "  names = []\n",
        "  global current_batch\n",
        "  cb = current_batch\n",
        "  if current_batch == -1:\n",
        "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!epoch finished!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "    return None\n",
        "  if cb+batch_size > len(test_data_names):\n",
        "    current_batch = cb + batch_size - len(test_data_names)\n",
        "    names = test_data_names[cb : len(test_data_names)] + test_data_names[0: current_batch ]\n",
        "    current_batch = -1\n",
        "  else:\n",
        "    names = test_data_names[cb : cb+batch_size]\n",
        "    current_batch = cb+batch_size\n",
        "  images = []\n",
        "  for name in names:\n",
        "    img = Image.open('baidu_star_2018_test_stage1/baidu_star_2018/image/preprocessed/'+name)\n",
        "    img = np.asarray(img)\n",
        "    img = MaxMinNormalization(img)\n",
        "    images.append(img)\n",
        "  return np.array(images), names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTmZYDbHKuCk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Run tests\"\"\"\n",
        "\n",
        "while True:\n",
        "  batch = next_batch(100)\n",
        "  if batch is not None:\n",
        "    images, names = batch\n",
        "    test_predict = inferencer.infer({'input': images})\n",
        "    test_predict = np.stack(test_predict,axis=1)\n",
        "    test_predict = np.transpose(test_predict, [0,2,1])\n",
        "    test_predict = np.reshape(test_predict, [batch_size, -1])\n",
        "    for m in range(len(names)):\n",
        "      test_data_dict[names[m]]['label'] = test_predict[m]\n",
        "  else:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4BLZxKY1OBPA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct results\"\"\"\n",
        "results = {}\n",
        "for name,item in test_data_dict.items():\n",
        "  predicted_label = item['label']\n",
        "  its_id = item['id']\n",
        "  num = 0\n",
        "  for i in range(0, len(predicted_label), 2):\n",
        "    pred_num = predicted_label[i+1]\n",
        "    num += pred_num\n",
        "  if results.has_key(its_id):\n",
        "    results[its_id] += num\n",
        "  else:\n",
        "    results[its_id] = num\n",
        "for key in results:\n",
        "  results[key] = int(round(results[key]))\n",
        "len(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgEPcnwWOGFV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Get results\"\"\"\n",
        "\n",
        "import csv\n",
        "with open('results.csv', 'wb') as csvfile:\n",
        "    rcsv = csv.writer( csvfile, dialect='excel' )\n",
        "    rcsv.writerow(['id', 'predicted'])\n",
        "    for (k,v) in results.items():\n",
        "      rcsv.writerow([k, v])\n",
        "      \n",
        "from google.colab import files\n",
        "files.download('results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}